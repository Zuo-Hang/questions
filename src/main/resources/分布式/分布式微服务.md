微服务：

服务注册与发现

服务调用

服务熔断

负载均衡

服务降级

服务消息队列

配置中心管理

服务网关

服务监控

全链路追踪

自动化部署

服务定时任务调度操作

spring cloud：一站式解决方案

服务的分类：

无业务基础服务：全局配置、ID自增器、短链接服务、文件存储服务、身份验证、邮件短信平台、语音回拨……

业务型基础模块：用户中心、账户中心、支付中心、信审中心、信息抓取系统、消息中心、活动广告、CMS……

最终springCloud通过网关调用负载均衡的每一个微服务。

版本：

imooc6666



### 理论：

##### CAP理论：（悖论）

* 数据一致性（consistency）：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有的读操作都不能读到这个数据，对调用者而言数据具有强一致性（strong consistency）
* 服务可用性（availability）：所有读写请求在一定时间内得到响应，可终止、不会一直等待
* 分区容错性（partition-tolerance）：在网络分区的情况下，被分隔的节点仍能正常对外服务

如果选择了CA而放弃了P，那么当发生分区现象时，为了保证C，系统需要禁止写入，当有写入请求时，系统返回error（例如，当前系统禁止写入），这又和A冲突了，因为A要求返回no error和no timeout。因此，分布式系统理论上不可能选择CA架构，只能选择CP或AP架构。

反证：

如果CAP三者可同时满足，由于允许P（网络分区）的存在，则一定存在节点之间丢包，如此则不能保证C

因为允许分区容错，写操作可能在1上成功，在节点2上失败，这时候对于client1（读取1节点）和client2读取2节点，就会读取到不一致的值，出现不一致的状况。如果要保持一致，写操作必须同时失败，也就是降低系统的可用性。

##### Base理论：（妥协）

cap理论的一种妥协，由于cap只能二取其一，base理论降低了发生分区容错时对可用性和一致性的要求。

1.基本可用：允许可用性降低（可能相应延长、可能服务降级）

2.软状态：指允许系统中的数据存在中间状态，并认为中间状态不会影响系统整体可用性。

3.最终一致性：节点数据同步可以存在延迟，但在一定的期限后必须达成数据的一致，状态变为最终状态。（很多种方案：因果一致、会话一致、单调一致……多种一致性模型）读写一致性

##### 数据一致性模型有哪些

强一致性：当更新操作完成之后，任何多个后续的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就能保证读到什么。根据cap理论，这种实现需要牺牲可用性。

弱一致性：系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。

最终一致性：最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致性，而不需要实时保证系统数据的强一致性。达到最终一致性的时间就是不一致性窗口时间，在没有故障的前提下，不一致性窗口的时间主要受通信延迟、系统负载和复制副本的个数影响。

最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等。



因果一致性：要求有因果关系的操作顺序执行得到保证，非因果关系的操作顺序则无所谓。

进程A在更新完某个数据后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取进程A更新后的最新值，并且如果进程B要对该数据进行更新操作的话，务必基于进程A更新后的最新值。

在微博或朋友圈进行评论的时候，比如你在朋友圈发了一整照片，朋友评论了，而你对朋友的评论进行了回复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。

会话一致性：将对系统数据的访问框定在了一个会话当中，约定了系统能过够保证在同一个有效会话中实现“读自己所写”的一致性，就是你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。实际开发过程中有分布式Session一致性问题，可以认为是会话一致性的一个应用。

单调一致性：

读写一致性：



##### 选举算法（主从架构中）Quorum、QARO机制

WARO：一种简单的副本控制协议，写操作时、只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机了，写服务就不会成功。但只要有一个节点存活，仍能提供读服务。（对应cpa的强一致性）kafka ack.o之中有使用其优化。

Quorum机制：10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据（10-3+1），可以保证读取到了最新的数据。无法保证强一致性，也就是无法保证任何实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。需要配合一个获取最新成功提交的版本号的metadata服务，这样可以确定最新已经耿工提交的版本号，然后从已经读取到的数据中就可以确认最新写入的数据。

##### Paxos算法：

思想，可以认为是一种协议。解决强一致性

Paxos算法解决的事是一个分布式系统如何让就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果每个节点的初始状态一致，每个节点执行相同的操作序列（指令+顺序），那么他们最后能够得到一个一致的状态。为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。在Paxos算法中，有三种角色：Proposer（提议者）、Acceptor（接受者）、Learners（记录员）

* Proposer提议者：只要Proposer发的提案Proposer被半数以上的Acceptor接受，Proposer就认为该提案的value被选定了。
* Acceptor接受者：只要Acceptor接受了某个提案，Acceptor就认为该提案的value被选定了
* Learner记录员：Acceptor告诉Learner哪个value被选定，Learner就认为哪个value被选定。

Paxos算法分为两个阶段，具体如下：

阶段一（preprae）：

1）Proposer接受client请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数Acceptor发送编号为N的Prepare请求。

2）Acceptor收到一个编号为N的Prepare请求，如果该轮Paxos

* 本节点已经有已提交的Value记录（持久化的），对比记录的编号和N，其值大于N则拒绝回应，否则返回该记录value及编号
* 没有已提交记录（没有持久化，但是接受了N1的其他提议），判断本地是否有编号N1，N1>N、则拒绝响应，否则将N1改为N（如果没有N1，则记录N），并相应prepare

阶段二（accept）：

1）如果proposer收到半数以上Acceptor对其发出的编号为N的Prepare相应的请求，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。V就是收到的相应中编号最大的Value，如果相应中不包含任何Value，那么V由Proposer自己决定。

2）如果Acceptor收到一个针对编号为N的提案的Accept请求，Acceptor对比本地的记录编号，如果小于等于N则接受该值，并提交记录Value。否则拒绝请求。



Proposer如果收到的大多数Acceptor响应，则选定Value值，并同步给Leaner，使未响应的Acceptor达成一致

![](分布式微服务.assets/QQ截图20210407182946.png)

活锁：accept时被拒绝，加大N，重新accept，此时另一个proposer也进行相同的操作，导致accept一致失败，无法完成算法。

multi-paxos：区别于paxos值时确定一个值，multi-paxos可以确定多个值，收到accept请求后，则一定时间内不再accept其他节点的请求，以此保证后续的编号不需要经过preprae确认，直接进入到accept操作。此时该节点成为leader，直到accept被拒绝，重新发起prepare请求竞争leader资格。



##### 简述raft算法：

概念：

* 分布式一致性算法：raft会先选举出leader，leader完全负责repliated log的管理。leader负责接受所有的客户端更新请求，然后复制到foller节点，并在“安全”的时候执行这些请求。如果leader故障，follwes会重新选举出新的leader
* 三种状态：一个节点任何一时刻处于三者之一
  * leader：处理所有的客户端请求（如果客户端将请求发给Follower，Follower将请求重定向给Leader）
  * follower：不会发送任何请求，只会简单地响应来自leader或Candidate的请求
  * candidate：用于选举产生新的leader（候选人）
* term：任期，leader产生到重新选取的一任期，每个节点都维持着当前的任期号
  * term是递增的，存储在log日志的entry中，代表当前entry是在哪一个term时期写入
  * 每个任期只能有一个leader或者没有（选举失败）
  * 每次rpc通信时传递该任期号，如果rpc收到任期号大于本地的、切换follower，小于本地任期号则返回错误信息
* 两个Rpc通信：
  * RequestVote Rpc ：负责选举，包含参数lastIndex、lastTerm
  * AppendEntries Rpc：负责数据交互
* 日志序列：每一个节点维持着一份持久化Log，通过一致性协议算法，保证一个节点中的log保持一致，并顺序存放，这样客户端就可以在每一个节点中读取到相同的数据。
* 状态机：日常序列同步到多数节点时，leader将该日志提交到状态机，并在下一次心跳通知所有节点提交状态机（携带最后提交的lastIndex）



何时触发选举：

* 集群初始化时，都是follower，随机超时，变成candidate，发起选举
* 如果follower在election timeout内没收到来自leader的心跳，则主动触发选举

选举过程：发出选举的节点角度

1、增加节点本地的term，切换到candidate状态

2、投自己一票

 		其他节点的投票逻辑：每个节点同一任期最多只能投一票，候选人知道的信息不能比自己少（通过副本日志和安全机制），先来先得

3、并行给其他节点发送RequestVote Rpcs（选举请求）、包含term参数

4、等待回复

​	4.1、收到majority（大多数）的投票，赢得选举，切换到leader状态，立刻给所有系欸但发心跳消息

​	4.2、被告知别人当选，切换到follower状态。（原来的leader对比term，比自己大，转换到follower状态）

​	4.3、一段时间没收到majority和leader的心跳通知，则保持candidate、重新发出选举



日志序列同步：日志需要存储在磁盘持久化，崩溃可以从日志恢复

1、客户端发送命令给Leader。

2、Leader把日志条目加到自己的日志序列里。

3、Leader发送AppendEntries Rpc请求给所有的follower。携带了prevLogIndex、prevLogTerm follower收到后，进行日志序列匹配

* 匹配上则之家到自己的日志序列
* 匹配不上则拒接请求，leader将日志index调小，重新同步直至匹配上，follower将leader的日志序列覆盖到本地



 一旦新的日志序列条目变成majority的了，将日志序列应用到状态机中

* Leader在状态机里提交自己日志序列条目，然后返回结果给客户端
* Leader下次发送AppendEntries Rpc时，告知佛来了微软已经提交的日志序列条目信息（lastIndex）
* follower收到RPC后，提交到自己的状态机里

提交状态机时，如果term为上一任期数据一起提交，否则可能出现覆盖已提交状态机的日志



新选举出的leader一定拥有所有已提交状态机的日志条数

* leader在当日志序列条目已经复制到大多数follower机器上时，才会提交日志条目。
* 而选出的leader的logIndex必须大于等于大多数节点，因此leader肯定有最新的日志



安全原则：

* 选举安全原则：对于一个给定的人气好，最多只会有一个leader被选取出来
* 状态机安全原则：如果一个leader已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志
* 领导人完全原则：如果某个日志条目在某个人气好中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中
* 领导人只附加原则：领导人结对不会删除或者覆盖自己的日志，只会增加
* 日志匹配原则：如果两个日志在相同的索引位置的日志的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同



##### 简述Zab协议

ZAB协议是为分布式协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议，实现分布式数据一致性

所有客户端的请求都是书写到Leader进程中，然后由leader同步到其他节点，成为follower。在集群数据同步的过程中，如果出现follower节点崩溃或者leader进程崩溃，都会通过zab协议来保证数据一致性

zab协议包含两种模式：崩溃恢复和消息广播。

消息广播：

集群中所有的事物请求都由leader节点来处理，其他服务器为follower，leader将客户端的事物请求转换为事物proposal，并且将proposal分发给集群中其他的所有follower

完成广播之后，leader等待follwer反馈，当由过半数的follower反馈信息后，leader将再次向集群内的follower广播commit信息，commit信息就是确认将之前的proposal提交。

leader节点的写入是一个两步操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数是指反馈节点数>=N/2+1，N是全部follower节点数量。



崩溃恢复：

* 初始化集群，刚刚启动的时候
* leader崩溃，因为故障宕机
* leader失去半数的机器支持，与集群中超过一半的节点断连

此时开启新一轮的leader选举，选举产生的leader会与过半的follower进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式，然后进入消息广播模式。



整个Zookeeper集群的一致性保证就是在上面两个状态之间切换，当leader服务正常时，就是正常的消息广播模式；当leader不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段。



Zxid 是zab协议的一个事物编号，zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端每一个事物请求，计数器加1；而高32位则代表leader周期年代的编号。

leader周期（epoch），可以理解为当前集群所处的年代或者周期，每当有一个新的leader选举出现时，就会从这个leader服务器上取出其本地日志中最大事物的zxid，并从中读取epoch值，然后加一，以此作为新的周期id。高32位代表了每代leader的唯一性，低32位则代表了每代leader中事物的唯一性。



zab节点的三种状态：

following：服从leader的命令

leading：负责协调事物

election、looking：选举状态



zab协议的一次选举肯定可以选出一个leader，但是reft协议不一定。



##### 负载均衡策略：

1.轮询法：

将请求按顺序轮询地分配到后端服务器上，它均衡的对待后端的每一台服务器，而不关心服务器实际链接数和当前的系统负载。

2.加权轮询法

不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此他们的抗压能力也不同。给配置高、负载低的机器分配更高的权重，让其处理更多的请求；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好的处理这一问题，并将请求顺序且按照权重分配到后端。

3.随机法

通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。

4.加权随机法

与加权轮询一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。

5.源地址哈希法

源地址哈希法的思想是根据客户端的ip地址，通过哈希函数计算得到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客户端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一ip地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。（解决了共享session的问题，但割裂了集群）

6.最小连接数法

最小链接数算法比较灵活和只能，由于后端服务器的配置不尽相同，对于请求的处理有块有慢，它是根据后端服务器当前的连接情况，动态的选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能的提高后端服务的利用率，将负责合理地分流到每一台服务器。

状态机：数据库、日志、



谁的值最先被大多数Accept，谁的值就会被持久化。

##### 集群、分布式、SOA、微服务的概念及区别

集群：不同服务器部署同一套应用对外提供访问，实现服务的负载均衡或者互备（热备、主从等），指同一种组件的多个实例，形成的逻辑上的整体。单个节点可以提供完整的服务。集群是物理形态。

分布式：服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，需要多个节点协调提供服务（也可以是相同组件部署在不同的节点，但节点间通过交换信息协作提供服务），分布式强调的是工作方式

SOA：面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在于操作系统进程中。各个服务之间通过网格调用。

* 中心化实现：ESB（企业服务总线），各服务通过ESB进行交互，解决异构系统之间的连通性，通过协议转换、消息解析、消息路由把服务提供者数据传送到服务消费者。很重，有一定的逻辑，可以解决一些公用逻辑的问题。
* 去中心化实现：微服务

微服务：在SOA上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成

服务单一职责

轻量级通信：去掉ESB总线，采用RestApi通信

##### 分布式系统的设计目标

可扩展性：通过对服务、存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。

高可用：单点不影响整体，单点故障指系统中某个组件一旦失效，会让整个系统无法工作

无状态：无状态的服务才能满足部分机器宕机不影响全部，可以随时进行扩展的需求

可管理：便于运维，出问题能不能及时发现定位

高可靠：同样的请求返回相同的数据；更新能够持久化；数据不会丢失

##### 分布式事物有哪些解决方案

基于XA协议：两阶段提交和三阶段提交，需要数据库层面支持（seata）

基于事物补偿机制的：TCC，基于业务层面实现

本地消息表：基于本地数据库+mq，维护本地状态（进行中），通过mq调用服务，完成后相应一条消息回调，将状态改成完成。需要配合定时任务扫表，重新发送消息调用服务，需要保证幂等（幂等性可以使用数据库主键这种方式来实现）

基于事物消息：mq （rabbitmq）

对比两阶段、三阶段有哪些改进

两阶段协议：

![QQ截图20210422120227](分布式微服务.assets/QQ截图20210422120227.png)

第一阶段（prepare）：每个参与者执行本地事物但不提交，进入ready状态，并通知协调者已经准备就绪

第二阶段（commit）：当协调者确认每个参与者都ready后，通知参与者进行commit操作；如果有参与者fail，则发送rollback命令，各参与者做回滚。

问题：

* 单点故障：一旦事务管理器出现故障，整个系统不可用（参与者都会阻塞住）
* 数据不一致：在阶段二，如果事务管理器只发送了部分commit消息，此时网络发生异常，那么只有部分参与者接受到commit消息，也就是说只有部分参与者提交了事物，使得系统数据不一致
* 相应时间较长：参与者和协调者资源都被锁住，提交或者回滚之后才能释放
* 不确定性：当写事物管理器发送commit之后，并且此时只有一个参与者收到了commit，那么当该参与者与事物管理器同时宕机之后，重新选举的事物管理器无法确定该条消息是否提交成功

三阶段协议：主要是针对两阶段的优化，解决了2PC单点故障的问题，但是性能问题和不一致问题依然没有根本解决

![QQ截图20210422180837](分布式微服务.assets/QQ截图20210422180837.png)

引入超时机制解决参与者阻塞的问题，超时后本地提交，2PC只有协调者有超时机制

* 第一阶段：canCommit阶段，协调者询问事物参与者，是否有能力完成此次事物。
  * 如果都返回yes，则进入第二阶段
  * 如果有一个返回no或等待响应超时，则中断事物，并向所有参与者发送abort请求
* 第二阶段：PreCommit阶段，此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事物操作。参与者执行完事物操作后（此时属于未提交事物的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待
* 第三阶段：DoCommit阶段，在阶段二中如果所有的参与者节点都返回了Ack，那么协调者就会从“预提交状态”转变为“提交状态”。然后向所有的参与者发送“doCommit”请求，参与者节点在搜到请求后会各自执行事物提交操作，并且向协调者节点反馈“Ack”消息，协调者收到所有参与者的Ack消息后完成事物。相反，如果有一个参与者节点未完成preCommit的反馈或者反馈超时时，那么协调者都会向所有的参与节点发送abort请求，从而中断事物。

##### 简述TCC事物模型：

TCC（补偿事物）：Try、Confirm、Cancel

针对每一个操作都要注册一个与其对应的确认和补偿（撤销）操作。

Try操作业务检测及资源预留，Confirm做业务确认操作，Cancel实现一个与Try相反的操作即回滚操作。TM首先发起所有的分支事物的try操作，任何一个分支事物的try操作执行失败，TM会将发起所有分支事物的Cancel操作，若try操作全部成功，TM将会发起所有分支事物的Confirm操作，其中Confirm/Cancel操作若执行失败，TM会进行重试。

TCC模型对业务的侵入性较强，改造难度较大，每个操作都需要Try、Confirm、Cancel三个接口实现

TCC中会添加事物日志，如果Confirm或者Cancel阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要仍共接入进行恢复和处理。

##### 如何理解RPC

远程过程调用

RPC要求在调用方中放置被调用的方法接口。调用方只要调用了这些接口，就相当于调用了被调用方的实际方法，十分易用。于是，调用方可以像调用内部接口一样调用远程的方法，而不用封装参数名和参数值等操作。

包含：

1. 动态代理、封装调用细节
2. 序列化与反序列化，数据传输与接受
3. 通信，可以选择七层HTTP、四层的TCP/IP
4. 异常处理等

首先，调用方调用的是接口，必须得为接口构造一个假的实现。显然，要使用到动态代理。这样，调用方的调用就被动态代理接收到了。

第二，动态代理接收到调用后，应该想办法调用远程的实际实现。这包括下面几步：

* 识别具体要调用的远程方法的IP、端口
* 将调用方法的入参进行序列化
* 通过通信将请求发送到远程的方法中

这样，远程的服务就接收到了调用方的请求。它应该：

* 反序列化各个调用参数
* 定位到实际要调用的方法、然后输入参数、执行方法
* 按照调用的路径返回调用结果

### Zookeeper

##### ZK的初始化选举和崩溃选举过程：

Zxid：事物id，SId：节点id

先对比Zxid，再对比SId，选票内容（zxid、sid），遇强改投

投票箱：每个节点在本地维护自己和其他节点的投票信息，改投时需要更新信息，并广播

节点状态：

* LOOKING，竞选状态。
* FOLLOWING，随从状态，同步leader状态，参与投票
* OBSERVING，观察状态，同步leader状态，不参与投票
* LEADING，领导者状态

初始化：没有历史数据，5个节点为例

* 节点1启动，此时只有一台服务器启动，它发出去的请求没有任何相应，所以它的选举状态一直是LOOKING状态
* 节点2启动，它与节点1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以serverId值较大的服务器节点2胜出，但是由于没有达到半数以上，所以服务器1、2还是保持LOOKING状态
* 节点3启动，与1、2节点通信交互数据，服务器3成为服务器1、2、3中的leader，此时有三台服务器选举了3，所以3成为leader
* 节点4启动，理论上服务器4应该是服务器1、2、3、4中serverId最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能切换到follower
* 节点5启动，同节点4一样

崩溃选举：

* 变更状态，leader故障后，follower进入LOOKING状态
* 各节点投票，先投自己（zxId、sId），在广播投票
* 接收到投票，对比zxId和sId，如果本节点小，则将票改为接收到的投票信息，并记录投票信息，重新广播。否则本节点打、则可不做处理
* 统计本地投票信息，超过半数，则切换为LEADING状态并广播。

##### 简述zk的数据模型

文件系统、树形结构。zk中的文件可以是文件也可以是文件目录。

每一个节点都是znode，

zk的数据模型是一种树形结构，具有一个固定的根节点（/），可以在根节点下创建子节点，并在子节点下继续创建下一级节点。每一层用/隔开，且只能用绝对路径（get /work/task1）的方式查询zk节点，而不能用相对路径。

* 持久节点：

  将节点创建为持久节点，该数据节点会一直存储在zk服务器上，即使创建该节点的客户端与服务器的会话关闭了，该节点依然不会被删除，除非显式调用delete函数进行删除操作。

* 临时节点：

  如果将节点创建为临时节点，那么该节点数据不会一直存储在zk服务器上。当创建该临时节点的客户端会话因超时或发生异常而关闭时，该节点也相应在zk服务器上被删除（基于一种session机制）。也可以主动调用delete删除。

* 有序节点：

  有序节点并不算一种单独类的节点，而是在持久节点和临时节点的基础上，增加一个节点有序的性质。创建有序节点的时候，zk服务器会自动使用一个单调递增的数字作为后缀，追加到创建的节点后边。例如一个客户端创建了一个路径为works/task-的有序节点，那么zookeeper将会生成一个序号并追加到该节点的路径后，最后该节点的路径为works/task-1。

节点内容：一个二进制数组（byte data[]），用来存储节点的数据、ACL访问控制（读写权限）、子节点数据（因为临时节点不允许有子节点，所以器子节点字段为null），记录自身状态信息的stat。

stat+节点路径可以查看状态信息

czxid：创建节点的事物id

mzxid：最后一次被更新的事物id

pzxid：子节点最后一次被修改的事物id

ctime：创建时间

mtime：最后更新时间

version：版本号、表示的是对节点数据内容、子节点信息或ACL信息的修改次数，可以不免并发更新问题，使用之前获取的版本号进行CAS操作更新（模仿的cas操作）。zk是一个文件系统，具有自己的数据库。

cversion：子节点版本号

aversion：acl的版本号

ephemeralOwner：创建节点的sessionId，如果是持久化节点，值为0

dataLeanght：数据内容长度

numChildren：子节点个数

##### ZK的数据同步原理

Leader——>F的数据同步

根据这三个参数的大小对比结果，选择对应的数据同步方式。

* peerLastZxid：learner服务器（Follwer或者Observer）最后处理zxid。
* minCommittedLog：Leader服务器proposal缓存队列committedLog中最小的zxid。
* maxCommittedLog：Leader服务器proposal缓存队列committedLog中的最大的zxid。

zookeeper中数据同步一共有四类，如下：

* DIFF：直接差异化同步

  peerlastZxid介于minCommittedLog和maxCommittedLog之间

* TRUNC+DIFF：先回滚再差异化同步

  当Leader服务器发现某个Learner包含了一条自己没有的事物记录，就要让该Learner进行事物回滚到Leader服务器上存在的。

* TRUNC：仅回滚同步

  peerLastZid大于maxCommittedLog，Leader会要求Learner回滚到Zxid值为maxCommitedLog对应的事物操作

* SNAP：全量同步

  peerLastZid小于minCommittedLog

在初始化阶段，Leader服务器会优先初始化以全量同步的同步方式来同步数据。

learner先向leader注册，上报peerlastZxid

##### ZK分布式锁实现原理

分布式锁：redis、数据库、ZK

在多个服务器上公用同一段代码，但是这段代码在同一个时间段内只能有一个机器运行。就需要分布式锁来保证安全性。

* 上来直接创建一个锁节点下的一个接一个的临时顺序节点。
* 如果自己不是第一个节点，就对上一个节点加一个监听器。
* 只要上一个节点释放了锁，自己就排到前面去了，相当于是一个排队机制。

而且用临时顺序节点，如果某个客户端创建临时顺序节点后，自己宕机了，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队。解决了惊群效应。

##### ZK的watch机制实现原理

##### 

```java
new Zookeeper(String connectString,int sessionTimeout,Watcher watcher)
这个watcher将作为整个zookeeper会话期间的上下文，一直被保存你在客户端ZKWatcherManager的defaultWatcher
```

也可以动态添加watcher：getData（），exists，getChildren

分布式环境下的观察者模式：通过客户端和服务端分别创建有观察者的信息列表。客户端调用相应接口时，首先将对应的Watch时间放在本地ZKWatcherManager中进行管理。服务端在接收到客户端的请求后根据请求类型判断是否含有watch事件，并将对应事件放在watchManager中进行管理。

在时间触发的时候服务端通过节点的路径信息查询相应的watch事件通知给客户端，客户端在接收到通知后，首先查询本地的ZKWatchManager获得对应的watch信息处理回调操作。这种设计不但实现了一个分布式环境下的观察者模式，而且通过将客户端和服务端各自处理watch事件所需的额外纤细分别保存在两端，减少彼此之间通信的内容。大大提升了服务的处理性能。



客户端实现过程：

* 标记该会话是一个带有watch事件的请求
* 通过DataWatchRegistration类来保存watcher事件的节点的对应关系
* 客户端向服务器发送请求，将请求封装成一个packet对象，并添加到一个等待发送队列outgoingQueue中
* 调用负责处理队列outgoingQueue的sendThread线程类中的readResponse方法接收服务端的回调，并在最后执行finishPacket（）方法将watch注册到ZKWatchManager，sendThread通过发送path路径和watcher为true，到server注册watch事件。

ZKWatchManager保存了Map<String,Set<Watcher>> dataWatchers、Map<String,Set<Watcher>> existsWatchers、Map<String ,Set<Watcher>> childrenWatchers三个集合，客户端会在dataWatchers中会添加一个key为path路径的本地事件



服务端实现过程：

* 解析收到的请求是否带有watch注册事件，通过FinalRequestProcessor类中的processRequest函数实现的。当getDataRequest.getWatch()值为true时，表明该请求需要进行Watch监控注册。
* 将对应的watch事件存储到watchManager，通过zks.getZKDatabase.getData函数实现，WatchManager该类中有HashMap<String,HashSetM<Watcher>> watchTable,key为path，Watcher是一个客户端网络链接封装，当节点变化时会通知对应的链接（链接通过心跳保持）



服务端触发过程：

* 调用WatchManager中的方法触发数据变更事件
* 封装了一个具有会话状态、事件类型、数据节点3中属性的watchedEvent对象。之后查询该系欸但注册的watch事件，如果为空说明该节点没有注册过watch事件。如果存在watch事件则添加到定义的watchers集合中，并在watchManager管理中删除。最后，通过调用process方法向客户端发送通知。

客户端回调过程：

* 使用SendThread.readResponse()方法来同意处理服务端的响应
* 将收到的字节流反序列化转换成WatcherEvent对象。调用eventThread.queueEvent（）方法将接收到的事件交给EventThread线程进行处理
* 从ZKWatchManager中查询注册过的客户端Watch信息。查询到后，会将其从ZKWatchManager的管理中删除。因此客户端的Watcher机制时一次性的，触发后就会被删除
* 将查询到的watcher存储到waitingEvents队列中，调用EventThread类中的run方法循环取出在waitingEvents队列中等待的watcher事件碱性处理

##### ZK的典型应用场景

通过对zookeeper中丰富的数据节点进行交叉使用，配合watcher时间通知机制，可以非常方便的构建一系列分布式应用会涉及到的核心功能，如：

1. 数据发布/订阅：配置中心
2. 负载均衡：提供服务者列表
3. 命名服务：提供服务名到服务地址的映射
4. 分布式协调、通知：watch机制和临时节点，获取各节点的任务进度，通过修改节点发出通知
5. 集群管理：是否有机器退出和加入、选举master
6. 分布式锁

第一类：在约定目录下创建临时目录节点，监听节点数目是否是要求的数目。

第二类：和分布式锁服务中的控制时序场景基本原理一直，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL节点，创建成功时watcher通知等待的队列，队列删除序列号最小的节点用以消费。此场景下Zookeeper的znode用于消息存储，znode存储的数据就是消息队列中消息内容，SEQUENTIAL序列号就是消息的编号，按序列取出即可。由于创建的节点是持久化的，所以不必担心队列消息丢失问题。

##### ZK中一个客户端修改了某个节点的数据，其他客户端能够马上就获取到这个最新的数据吗？

有两种可能：

因为ZK存在数据同步机制，所以客户端在没有特定操作的时候是有可能获取不到的（访问了F节点）

但是ZK客户端提供了一个sync函数，先调用这个函数再进行数据的获取，就可以获取到最新数据了（访问量L节点）

##### ZK的会话管理机制：

ZK和客户端Client之间的Session的机制，服务端会维护一个sessionId。当会话过期的时候就会删除此session创建的临时节点。

客户端链接zk，由zk分配一个全局唯一的sessionId，客户端需要配置超时时间timeOut并传到zk，zk会据此计算会话下一次超时的时间点，zk根据这个时间点按照分桶策略进行分来存放，zk会给session设置一个isClosing属性、如果检测到超时会将该属性标记为关闭

会话状态：CONNECTING、CONNECTED、RECONNECTING、RECONNECTED、CLOSE

sessionTracker：zk中的会话管理器，负责会话的创建、管理和清理

* sessionsWithTimeout：一个ConcurrentHashMap，用来管理会话的超时时间
* sessionsById：HashMap，维护sessionId到session的映射
* sessionsSets：HashMap，会话超时后进行归档，便于恢复和管理

ExpiractionTime=CurrentTime+SessionTimeout

SessionTracker根据ExpiractionTime将session进行粪桶管理，同事按照一定的时间间隔进行定期检查，客户端读写请求都可以将session的超时时间重置，sessionTracker会将session进行分桶迁移，如果没有读写请求，客户西药发送ping心跳链接，否则session超时会被清除。

会话清理：

* 标记isClosing为关闭，此时该会话有新的请求也无法处理
* 发起会话关闭请求，同步到整个集群，使用提交的方式
* 收集需要清理的临时节点，先获取内存数据库中会话对应的临时节点集合，如果此时有删除节点的请求到达，将请求的节点路径从集合中移除，避免重复删除，如果有创建节点请求到达，则将请求中的路径添加到集合中
* 添加删除事物变更，将节点删除事物添加到outstandingChanges中，触发watch
* 删除临时节点
* 移除会话
* 关闭链接

链接断开后客户端可以重连zk，如果该session未过期，session重新变为CONNECTED

如果时间超过sessionTimeOut，服务器则会进行会话的清理工作，如果此时zk客户端才恢复链接，则会收到State为Expired的WatchEvent，并断开与服务器的链接。

重连：断开后更换服务器链接，RECONNECTING状态，会将会话迁移到新链接的服务器上

当一个客户端发一个心跳请求服务端，但是网络延迟，导致服务端没有收到，过了一会儿，客户端连接上了另一个新的服务端，在这之后，之前的心跳被旧的服务端收到了，这时候旧的服务端会被提醒，当前session已经被转移了，然后旧的服务端会关闭这个链接。客户端一般不会感知到这个异常，因为旧链接一般都会被关闭。但是还有一个特殊情况，两个客户端同时使用保存着的sessionId+密码来重连服务端，第一个链接成功，紧接着第二个有连接成功，这就会导致第一个链接关闭，然后就是这两个客户端无限重连。

##### ZK对事物性的支持

Zookeeper对于事务性的支持主要依赖四个函数：zoo_creat_op_init、zoo_delete_op_init、zoo_set_op_init以及zoo_check_op_init

每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations。当准备好一个事物中的所有操作后，可以使用zoo_multi来提交所有的操作，由zookeeper服务来保证这一系列操作的原子性。也就是说只要其中有一个操作失败了，相当于此次提交的任何一个操作都没有对服务端的数据造成影响。zoo_multi的返回值是第一个失败操作的状态信号。

##### ZK中的观察者机制

```
peerType=observer
server.1:localhost:2181:3181:observer
```

观察者的设计是希望能够动态扩展zookeeper集群又不会降低读写性能。

如果扩展节点是follower，则写入操作提交时需要同步的节点数会变多，导致写性能下降，而follower又是参与投票的、也会导致投票成本增加

observer是一种新的节点类型，解决扩展问题的同事，不参与投票，只获取投票结果，同事也可以处理读写请求，写请求转发给leader。负责接收leader同步过来的提交数据，observer的节点故障也不会影响集群的可用性，跨数据中心部署。把节点分散到多个数据中心可能因为网络的延迟会极大的拖慢系统。使用observer的话，更新操作都在一个单独的数据中心来处理，并发送到其他数据中心，让其他数据中心的节点消费数据。

无法完全消除数据中心之间的网络延迟，因为observer需要把更新请求转发到另一个数据中心的leader，并处理同步消息，网络速度极慢的话也会有影响，它的优势是为本地读请求提供快速响应。

##### zookeeper CP

zookeeper在选举leader时，会停止服务，直到选举成功之后才会再次对外提供服务，这个时候就说明了服务不可用，但是在选举成功之后，因为一主多从的结构，zookeeper在这时还是一个高可用注册中心，只是在优先保证一致性的前提下，zookeeper才会顾及到可用性

##### 2.1 zookeeper 应用场景

- 感知消息队列异步操作后的结果

- 分布式锁

- 元数据 或者配置中心 如 dubbo 和Kafka 都需要zookeeper

  - > dubbo 也可以不使用zookeeper 采用直连提供的方式，但限制了分布式的拓展性。

- HA高可用

  - 主备切换 (两个服务分别为主备，备用平时不提供服务，当主的挂掉后，备用顶上作为新主。当原来的主恢复后作为新备)

**选型依据**：

在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。

但是在交易场景交易链路上，在主业务数据存取，大规模服务发现、大规模健康监测等方面有天然的短板，应该竭力避免在这些场景下引入 ZooKeeper，在阿里巴巴的生产实践中，应用对 ZooKeeper 申请使用的时候要进行严格的场景、容量、SLA 需求的评估。

所以可以使用 ZooKeeper，但是大数据请向左，而交易则向右，分布式协调向左，服务发现向右。

##### 如何实现分表分库：

将原本存储在单个数据库上的数据拆分到多个数据库，把原来存储在单张数据表的数据拆分到多张数据表中，实现数据切分，从而提升数据库性能。分库分表的实现可以分为两种方式：垂直切分和水平切分

水平：将数据分散到多张表，涉及分区键：分区键可以使用哈希、一致性哈希、主键等

* 分库：每个库结构一样，数据不一样，没有交集。库多了可以缓解io和cpu压力
* 分表：每个表结构一样，数据不一样，没有交集。表数量减少可以提高sql执行效率、减轻cpu压力

垂直：将字段拆分为多张表，需要一定的重构，比如将热点字段提为新表新库。

* 分库：每个库结构、数据都不一样，所有库的交集为全量数据
* 分表：每个表结构、数据不一样，至少有一列交集，用于关联数据，所有表的集合为全量数据

但是分表分库会带来很多的问题，如分布式事物、主键的问题等。要根据业务决定是否进行。

### Dubbo

##### Dubbo是什么？能做什么？

Dubbo是阿里巴巴开源的基于Java的高性能功能RPC分布式服务框架，现在已经成为Apache基金的项目。致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。

简单的说，dubbo就是个服务框架，如果没有分布式的需求，其实是不需要用的，只有在分布式的时候，才有dubbo这样的分布式服务框架的需求，本质上是个远程调用的分布式框架。

其核心部分包含：

1. 远程通讯：通过对多种基于长连接的NIO框架抽象封装，包括多种线程模型，序列化，以及”响应-请求“模式的信息交换方式，透明化的远程方法调用，就像调用本地方法一样调用远程方法，只需简单的配置，没有任何API侵入。
2. 集群容错：提供基于接口方法的透明远程过程调用，包括多协议支持，以及负载均衡，失败容错，地址路由，动态配置等集群支持，可在内网代替F等硬件负载均衡，降低成本，减少单点。
3. 自动发现：基于注册中心目录服务，服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加删除服务提供者。

##### Dubbo的工作流程：

1. start：启动spring容器时，自动启动Dubbo的provider
2. register：dubbo的provider在启动后会去注册中心注册内容。注册的内容包括IP、端口号、接口列表（接口类、方法）、版本号、Provider的协议
3. Subscribe：订阅。当Consumer启动时，自动去Registry获取到所已注册的服务信息。
4. Notify：通知。当Provider的信息发生变化的时候，自动由registry向Consume推送通知。
5. Invoke：Consumer调用Provider中的方法
   * 同步请求，消耗一定的性能，但是必须是同步请求，因为需要接收调用方法后的结果
6. Count：次数，每隔2分钟，provider和consumer自动向Monitor发送访问次数。Monitor进行统计。

##### dubbo中的SPI

SPI：service provider interface 服务提供接口  就是一个服务发现机制，通过接口全限定名找到指定目录下对应的文件，获取具体的实现类然后加载即可，做到了灵活的替换具体的实现类。

Jdk中也有spi机制，但是主要用于数据库驱动的加载过程。

![QQ截图20210516203358](分布式微服务.assets/QQ截图20210516203358.png)

##### 为什么dubbo不适用jdk中的spi，而是要自己实现

Java spi缺点：

1. 需要遍历所有实现并实例化，假设一个实现类初始化过程比较消耗资源且耗时，但是你的代码里面又用不上它，这时候就产生了资源浪费。也无法准确引用。
2. 没有缓存机制，每次load都需要重新加载。

Dubbo SPI：

1. 给每个实现类配了一个名字，通过名字去文件里面找到对应的实现类全限定名然后加载实例化，按需加载。
2. 增加了缓存存储实例，提高读取性能。
3. 提供了对IOC和AOP等高级功能的支持，以实现更多类型的扩展。

##### dubbo服务暴露过程

Dubbo采用URL的方式来作为约定的参数类型。

```
protocol://username:password@host:port/path?key=value&key=value
protocol:指的是dubbo中的各种协议，如：dubbo thrift http
username/password:用户名/密码
host/port:主机/端口
paramters:参数键值对
```

ServiceBean实现了ApplicationListener，监听ContextRefreshedEvent事件，在spring IOC容器刷新完成后调用 onApplicationEvent方法，服务暴露的启动点。根据配置得到URL，再利用Dubbo SPI机制根据URL的参数选择对应的实现类，实现扩展。

通过Javassist动态封装服务实现类，统一暴露出Invoker使得调用方便，屏蔽底层实现细节，然后封装成exporter存储起来，等待消费者的调用，并且会将URL注册到注册中心，使得消费者可以获取服务提供者的信息。

一个服务如果有多个协议那么就都需要暴露，比如同时支持dubbo协议和hessian协议，那么需要将这个服务用两种协议分别向多个注册中心（如果有多个的话）暴露注册。

1. 检测配置，如果有些配置为空的话会默认创建，并且组装成URL
2. 根据URL进行服务暴露、创建代理类invoker、根据URL得知具体的协议，根据Dubbo SPI选取实现类来exporter
3. 如果只是本地暴露，将exporter存入serviceConfig的缓存。
4. 远程暴露，先通过registry协议找到RegistryProtocol进行export，将URL中export=dubbo：//……先转换成exporter，然后获取注册中心的相关配置，如果需要注册就向注册中心注册，并且在ProviderConsumerRegTable这个表格中记录服务提供者，其实就是往一个ConcurrentHashMap中将塞入invoker，key就是服务接口全限定名，value是一个set，set里面会存包装过的invoker，根据URL上Dubbo协议暴露出exporter，打开server调用NettyServer来监听服务。

##### Dubbo服务引入过程

饿汉式就是通过调用ReferenceBean的afterPropertiesSet方法时引入服务。

懒汉式是只有当这个服务被注入到其他类中时启动引入流程，也就是说用到了才会开始服务引入。默认使用懒汉式，如果需要使用饿汉式，可以通过配置dubbo:reference的init属性开启。

ReferenceBean实现了FactoryBean接口，当对任意服务Interface进行自动注入或者getBean获取时，就会触发getObject（）函数的服务引用过程。

* 本地引用走injvm协议，到服务暴露的缓存中取exporter
* 直连远程引入服务，测试情况下不需要启动注册中心，由consumer直接配置写死provider的地址，然后直连即可。
* 注册中心引入远程服务，Consumer通过注册中心得知provider的相关信息，然后进行服务的引入。

获取注册中心实例，向注册中心注册自身，并订阅providers、configurators、routers节点，触发DubboInvoker的生成，Cluster将多个服务提供者进行封装，返回一个invoker

通过配置构建一个map，然后利用map来构建URL，再通过URL上的协议利用自适应扩展机制用对应的protocol.refer得到相应的invoker，然后再构建代理，封装invoker返回服务引用，之后consumer调用这个代理类。

##### Dubbo服务调用过程

调用某个接口的方法会调用之前生成的代理类，然后会从cluster中经过路由的过滤、负载均衡机制随机选择一个invoker发起远程调用，此时会记录此请求和请求id等待服务端的响应。

服务端接收请求之后会通过参数找到之前暴露存储的map，得到响应的exporter，然后最终调用真正的实现类，再组装好结果返回，这个响应会带上之前请求的ID

消费者收到这个响应之后会通过ID去找之前记录的请求，然后找到请求之后将响应塞到对应的Future中，唤醒等待线程，最后消费者得到响应。

##### Dubbo中ZK集群挂掉了，发布者和订阅者还能进行通信吗？

可以。

因为当启动dubbo容器时，消费者会去zookeeper拉去注册的生产者的地址列表，并将其缓存到本地。每次发起调用时，都会按照本地的地址列表，以负载均衡的策略去进行调用。但是zookeeper挂掉后则后续新的生产者无法被消费者发现。

* 注册中心对等集群，任意一台宕掉后，会自动切换到另一台
* 注册中心全部宕机，服务提供者和消费者仍可以通过本地缓存通讯
* 服务提供者无状态，任一台宕机后，不影响使用
* 服务提供者全部宕机，服务消费者会无法使用，并无限重连等待服务者恢复

##### ZK实际如何存储dubbo生产者和消费者信息

##### Dubbo支持的注册中心有哪些？

zookeeper（官方推荐）

* 优点：支持分布式
* 缺点：受制于zookeeper的特性

Multicast：组播协议允许将一台主机发送的数据据通过网络路由器和交换机复制到多个加入此组播的主机，是一种一对多的通讯方式。每一台服务器提供方和消费方都可以看作是注册中心的一部分

* 优点：去中心化，不需要单独安装软件
* 缺点：provider和consumer和registry不能跨机房（路由）

不需要启动任何中心节点，只要广播地址一样，就可以相互发现，组播受网络结构限制，知识和小规模应用或者开发阶段使用。

Redis

* 优点：支持集群，性能高
* 缺点：要求服务器时间同步，否则可能出现集群失败问题

Simple

* 优点：标准RPC服务，没有兼容问题
* 缺点：不支持集群

##### Dubbo集群容错策略有哪些？

Failover Cluster失败自动切换：dubbo的默认容错方案，当调用失败的时候自动切换到其他可用节点，具体的重试次数和时间间隔可以通过引用服务的时候配置，默认重试次数为1，也就是只调用一次。

Failback Cluster失败自动恢复：在调用失败，记录日志和调用信息，然后返回空结果给Consumer，并且通过定时任务每隔5秒对失败的调用进行重试

Failfast Cluster快速失败：只会调用一次，失败之后立刻抛出异常

Failsafe Cluster失败安全：调用出现异常，记录日志不抛出异常，返回空结果。

Forking Cluster并行调用多个服务提供者：通过线程池创建多个线程，并发调用多个provider，结果保存到阻塞队列，只要有一个provider成功返回了结果，就立即返回结果。

Broadcast Cluster广播模式：逐个调用每个provider，如果其中一台报错，在循环调用结束后，抛出异常。

##### Dubbo支持的协议有哪些？

Dubbo协议（官方推荐）

* 采用NIO复用单一长连接，并使用线程池处理并发请求，减少握手和加大并发效率，性能较好（推荐使用）
* 大文件上传时，可能出现问题（不适用Dubbo文件上传）

RMI协议（Remote Method Invocation）协议

* jdk自带的能力，可与原生RMI互操作，基于TCP协议、短链接

Hessian协议

* 可与原生Hessian互操作，基于Http协议，对于数据包比较大的情况比较友好
* 需要Hessian.jar支持，Http短链接开销大，它的参数和返回值都需要实现Serializable接口

Http协议

Webservice：基于CXF的Fronted-simple和Transports-http实现；基于webService的远程调用协议。

* 序列化：SOAP文本序列化
* 适用场景：系统集成，跨语言调用

Thrif：Thrift是Facebook捐给Apache的一个RPC框架，特点是语言中立、平台中立。

##### Dubbo的分层设计

![QQ截图20210516220613](分布式微服务.assets/QQ截图20210516220613.png)

* Service：业务层，就是咱们开发的业务逻辑层
* Config，配置层，主要围绕ServiceConfig和ReferenceConfig，初始化配置信息。
* Proxy，代理层，服务提供者还是消费者都会生成一个代理，使得服务接口透明化，代理层做远程调用和返回结果
* Register，注册层，封装了服务注册和发现
* Cluster，路由和集群容错层，负责选取具体调用点，处理特殊的调用要求和负责远程调用失败的容错措施。
* Monitor，监控层，负责监控统计调用时间和次数
* Portocol，远程调用层，主要是封装RPC调用，主要负责管理Invoker。
* Exchange，信息交换层，用来封装请求响应模型，同步转异步
* Transport，网络传输层，抽象了网络传输的统一接口，Netty、Mina等
* Serialize，序列化层，将数据序列化成为二进制流，以及反序列化。

##### Dubbo和Spring-Cloud的对比

1. dubbo由于是二进制的传输，占用带宽会更少。
2. spring-cloud是HTTP协议传输，带宽会比较多，同时使用Http协议一般会使用JSON报文，消耗会更大
3. dubbo的开发难度较大，原因是dubbo的jar包依赖问题很多，大型项目很难解决
4. spring-cloud的接口协议约定比较自由松散，需要有强有力的行政措施来限制接口无序升级
5. dubbo只是spring-cloud的一个子集，解决的是分布式中服务调用的问题，而spring-cloud提供了全套的解决方案。

### Redis

##### 如何避免缓存雪崩、缓存击穿、缓存穿透

缓存雪崩是指缓存同一时间内大面积失效，所以、后面的请求都会落到数据库上面，造成数据库短时间内承受大量的请求而崩掉。

解决方案：

* 缓存数据的过期时间设置随机值，防止同一时间大量数据过期现象发生。
* 给每一个缓存数据增加响应的缓存标记，记录缓存是否失效，如果缓存标记失效，则更新数据缓存。
* 缓存预热
* 互斥锁（业务层面，查完缓存，查数据库的时候加分布式锁）

缓存穿透是指缓存和数据库中都没有数据，导致所有的请求都落到了数据库上，造成数据库短时间内承受大量请求而崩掉。

解决方案：

* 接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截；
* 从缓存获取不到的数据，在数据库中也没有找到，这时可以将key-value对写成key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况下也没办法使用，并且占用内存）。这样可以防止用户反复用同一个id暴力攻击。
* 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

缓存击穿是指缓存中没有数据但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没有读到数据，有同时去数据库读取数据，引起数据库压力瞬间增大，造成过大压力。和缓存不同的是，缓存击穿指并发查询同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。

解决方案

* 设置热点数据永远不过期
* 加互斥锁

##### 分布式系统中常用的缓存方案有哪些

客户端缓存：页面和浏览器缓存，APP缓存，H5缓存，LocaStorage和SessionStorage

DNS缓存：内容存储：数据的缓存，内容分发：负载均衡

Nginx缓存：静态资源

服务端缓存：本地缓存，外部缓存

数据库缓存：持久层（mybatis、hibernate多级缓存），mysql查询缓存

操作系统缓存：Page cache、Buffer cache

##### 缓存过期都有哪些策略？

定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。

惰性过期：当有访问一个key时，才会判断该key是否过期，过期则清除。该策略可以最大化地节省CPU资源，但是很消耗内存、许多的过期数据都还存在内存中。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。

定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key（是随机的），并清除其中已经过期的key。该策略是定时过期和惰性过期的这种方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况 下使得CPU和内存资源达到最优的平衡效果。

##### 常见的缓存淘汰算法：

FIFO（First In First Out，先进先出），根据缓存被存储的时间，离当前最远的数据优先被淘汰；

LRU（Least Recently Used，最近最少使用），根据最近被使用的时间，离当前最远的数据优先被淘汰；

LFU（Least Frequently Used，最不经常使用），在一段时间内，缓存数据被使用次数最少的会被淘汰。

##### 如何保证数据库与缓存的一致性

由于缓存和数据库是分开的，无法做到原子性的同时进行数据修改，可能会出现缓存更新失败，或者数据库更新失败的情况，这时候会出现数据不一致，影响前端业务。

* 先更新数据库，再更新缓存。缓存可能更新失败，读到老数据

* 先删除缓存，再更新数据库。并发时，读操作可能还是会将旧数据读回缓存

  ```
  经典的缓存+数据库读写的模式，Cache Aside Pattern
  读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后存放到缓存，同时返回响应。
  更新的时候，先更新诗剧苦，然后再删除缓存。
  ```

为什么经典策略是删除而不是更新？

删除更加清凉，延迟加载的一种实现，更新可能设计多个表，比较耗时



延时双删：先删除缓存，再更新数据据库，休眠1s、再次删除缓存。写数据的休眠时间则在读数据业务逻辑的耗时基础上，再加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据，并发还是可能读到旧值覆盖缓存。

终极方案：

将访问操作串行化

1. 先删除缓存，将更新数据库的操作放进有序队列中。
2. 从缓存查不到的查询操作，都进入有序队列

会面临的问题：

1. 读请求积压，大量查实，导致数据库的压力：限流、熔断
2. 如何避免大量请求积压：将队列水平拆分，提高并行度
3. 保证相同请求路由正确

##### 布隆过滤器原理，优缺点

位图：int[10]每个int类型的整数时4*8=32个bit，则int[10]一共有320bit，每个bit非0即1，初始化时都是0

添加数据时，将数据进行hash得到hash值，对应到bit位，将该bit改为1，hash函数可以定义多个，则一个数据添加会将多个（hash函数个数）bit改成1，多个hash函数的目的是减少hash碰撞的概率

查询数据：hash函数计算得到hash值，对应到bit中，如果有一个为0，则说明数据不在bit中，如果都为1，则该数据可能在bit中



优点：

* 占用内存小
* 增加和查询元素的时间复杂度为O（k），k为哈希函数的个数，一般比较小，与数据量大小无关
* 哈希函数互相之间没有关系，方便硬件并行运算
* 布隆过滤器不需要存储元素本身，在某些对保密要求比较严格的场合有很大优势
* 数据量很大时，布隆过滤器可以表示全集
* 使用同一组散列函数的布隆过滤器可以进行交、并、差运算。

缺点：

* 误判率，即存在假阳性（False Position），不能准确判断元素是否在集合中
* 不能获取元素本身
* 一般情况不能从布隆过滤器中删除元素

##### 什么是服务降级、什么是服务熔断

降级时解决系统资源不足和海量请求之间的矛盾

在 暴增的流量请求下，对一些非核心流程业务、非关键业务，进行有策略的放弃，以此来释放系统资源，保证核心业务的正常运行，尽量避免这种系统资源分配的不平衡，打破二八策略，让更多的机器资源，承载主要业务请求。服务降级不是一个常态策略，而是对应非正常情况下的应急策略。服务降级的结果，通常时对一些业务请求返回一个统一结果，可以理解成一种FailOver快速失败策略。一般通过配置中心开关实现开启降级



熔断模式保护的是业务系统不被外部大流量或者下游系统的异常而拖垮。

如果开启了熔断，订单服务可以在下游调用出现异常时，调节流量请求，比如出现10%的失败之后，减少50%的流量请求，如果继续出现50%的异常，则减少80%的流量请求；相应的，在检查的下游服务正常后，首先恢复30%的流量，然后是50%的流量，接下来是全部流量

##### 高并发场景下如何实现系统限流

限流一般需要结合容量规划和压测来进行。当外部请求接近或者达到系统的最大阈值时，触发限流，采取其他的手段进行降级，保护系统不被压垮。常见的降级策略包括延迟处理、拒绝服务、随机拒绝等。

计数器法：

1. 将时间划分为固定的窗口大小，例如1s
2. 在窗口时间段内，每来一个请求，对计数器加1
3. 当计数器达到设定限制后，该敞口时间内的之后的请求都被丢弃处理
4. 该敞口时间结束后，计数器清零，从新开始计数

滑动敞口计数法：

1. 将时间划分为细粒度的区间，每个区间维持一个计数器，每进入一个请求则将计数器加1
2. 多个区间组成一个时间窗口，每六十一个区间时间后，则抛弃最老的一个区间，纳入新区间。
3. 若当前窗口的区间计数器总和超过设定的限制数量，则本窗口的后续请求都被丢弃。

漏桶算法：如果外部请求数量超出当前阈值，则会在容器里继续，一直到溢出，系统并不关心移除的流量。从出口处限制请求速率，并不存在计数器法的临界问题，请求曲线始终是平稳的。无法应对突发流量，相当于一个空同+固定处理线程。

令牌桶算法：假设一个大小恒定的同，这个桶的容量和设定的阈值有关，同里面存放着很多的令牌，通过一个固定速率向桶里存放令牌。如果桶满了，就把令牌丢掉，最后后同中可以保存的最大令牌数永远不会超过桶的容量，当有请求进入的时候，就尝试从桶里取走一个令牌，如果桶是空的，那么这个请求会被拒绝。

##### Redis的持久化机制

RDB：redis database 将某一时刻的内存快照（Snaphot），以二进制的方式写入磁盘。

手动触发：

* save命令：使redis处于阻塞状态，知道RDB完成，才会响应其他客户端发来的命令，所以在生产环境下一定要慎用
* bgsave命令：fork出一个子进程执行持久化，主进程只在fork的过程中有短暂的阻塞，子进程创建之后，主进程就可以响应客户端请求了（在子进程未完成持久化的过程中，使用写时复制来保证镜像的准确性）

自动触发：

* save m n：在m秒内，如果有n个键发生改变，则自动触发持久化，通过bgsave执行，如果设置了多个，只要满足其中一个就会触发，配置文件有默认配置（可以注释掉）
* flushall：用于清空redis所有的数据库，flushadb清空当前redis所在数据库（默认是0号数据库），会清空RDB文件，同时也会生成dump.rdb、内容为空
* 主从同步：全量同步时会自动触发bgsave命令，生成rdb文件发送给从节点。

优点：

1. 整个redis数据库将只包含一个dump.rdb，方便持久化
2. 容灾性好，方便备份
3. 性能最大化，fork子进程来完成写操作，让主进程继续处理命令，所以是IO最大化。使用单独子进程来进行持久化，主进程不会进行任何IO操作，保证redis的高性能
4. 相对于数据集大时，比AOF的启动效率更高

缺点：

1. 数据安全性低。RDB时间隔一段时间进行持久化，如果持久花花之间redis发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候。
2. 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至1分钟。会占用CPU



AOF：Append Only File 以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录，调用操作系统命令进程刷盘

1. 所有的命令会追加到AOF缓冲中。
2. AOF缓冲区根据对应的策略向硬盘进行同步操作。
3. 随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。
4. 当redis重启时，可以加载AOF文件进行数据恢复。

同步策略：

* 每秒同步：异步完成，效率非常高，一旦系统宕机，那么这一秒之内修改的数据将丢失
* 每修改同步：同步持久化，每次发生的数据变化都会被记录到磁盘，最多丢失一条
* 不同步：由操作系统控制，可能丢失较多的数据

优点：

1. 数据安全
2. 通过append模式写文件，即使中途服务器宕机也不会破坏已经存在的内容，可以通过redis-check-aof工具解决数据一致性问题。
3. AOF机制的rewrite模式，定期对AOP文件进行重写，以达到压缩的目的。

缺点：

1. AOF文件比RDB文件大，且恢复速度慢。
2. 数据集大的时候，比rdb启动效率低。
3. 运行效率没有RDB高



AOF文件比RDB更新频率高，优先使用AOF还原数据。

AOF也比RDB安全性好

RDB性能比AOF好

两者都配置了的话优先加载AOF

##### Redis单线程为什么这么快？

Redis基于Reactor模式开发了网络事件处理器、文件监听处理器 file event handler。他是单线程的，所以redis才叫做单线程的模型，它采用了IO多路复用机制来同时监听了多个Socket，根据Socket上的事件类型来选择对应的事件处理器来处理这个事件。可以实现高性能的网络通信模型，由可以跟内部其他单线程的模块进行对接，保证了redis内部的线程模型的简单。

文件事件处理器的结构包含四个部分：多个socket，IO多路复用程序、文件事件分派器以及事件处理器（命令请求处理器、命令回复处理器、连接应答处理器等）。

每个Socket可能并发的产生不同的事件，IO多路复用程序会监听多个Socket，会将Socket放入一个队列中排队，每次从队列中有序、同步取出一个Socket给事件分派器，事件分派其把Socket给对应的事件处理器。然后一个Socket的事件处理完之后，IO多路复用程序才会将队列中的下一个Socket给事件分派器。文件事件分派器会根据每个Socket当前产生的事件，来选择对应的事件处理器来处理。



1. Redis启动初始化时，将链接应答处理器跟AE_READABLE事件关联。
2. 若一个客户端发起链接，会产生一个AE_READABLE事件，然后由链接应答处理器负责和客户端建立连接，创建客户端对应的Socket，同时将这个socket的AE_READABLE事件和命令请求处理器关联，使得客户端可以向主服务器发送命令请求。
3. 当客户端向Redis发请求时（不管读请求还是写请求），客户端socket都会产生一个AE_READABLE事件，触发命令请求处理器。处理器读取客户端的命令内容，然后传给相关程序执行。
4. 当Redis服务器准备好给客户端的响应数据后，会将socket的AE_WRITABLE事件和命令回复处理器关联，当客户端准备好读取响应数据时，会在socket产生一个AE_WRITABLE事件，由对应命令回复处理器处理，即将准备好的响应数据写入socket，共客户端读取。
5. 命令回复处理器全部写完到socket后，就会删除该socket的AE_WRITABLE事件和命令回复处理器的映射。



单线程快的原因：

1. 纯内存操作
2. 核心是基于非阻塞IO多路复用机制
3. 单线程反而避免了多线程的频繁上下文切换带来的性能问题



##### Redis主从同步机制

1. 从系欸但执行slaveof masterIp port，保存主节点信息
2. 从节点中的定时任务发现主节点信息，建立和主节点的socket链接
3. 从节点发送信号，主节点返回，两边能互相通信
4. 连接建立后，主节点将所有数据发送给从节点（数据同步）
5. 主节点把当前数据给同步节点后，便完成了复制过程。接下来，主节点就会持续的把写命令发送给从节点，保证主从数据一致性。



runId：每个redis系欸但启动都会生成唯一uuid，每次redis重启之后，runId都会发生变化。

offset：主节点各自维护自己的复制偏移量offset，当主节点有写入命令时，offset=offset+命令的字节长度。从节点在收到主节点发送的命令之后，也会增加自己的offset，并把自己的offset发送给主节点。主节点同时保存自己的offset和从节点的offset，通过对比offset来判断主节点数据是否一致。

repl_backlog_size：保存在主节点上的一个固定长度的先进先出队列，默认大小是1MB



全量复制：

* 主节点发送psync命令，psync runid offset（由于是第一次，runId为？，offset为-1）
* 主节点返回FULLRESYNC runId offset，runId是主节点的runId，offset是住系欸但那目前的offset。主从节点保存信息
* 主节点启动bgsave命令fork子进程进行RDB持久化
* 主节点将RDB文件发送给从节点，到从系欸但加载完数据前，写命令写入缓冲区
* 从系欸但清理本地数据并加载RDB，如果开启了AOF会从写AOF



部分复制：

1. 复制偏移量：psync runId offset
2. 复制积压缓冲区：当主节点offset的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。
   * 如果从系欸但保存的runId与主节点现在的runId相同，说明主节点之前同步过，主节点会持续尝试使用部分复制（到底能不能部分复制还要看offset和复制积压缓冲区的情况）；
   * 如果主节点保存的runId与主节点现在的runId不同，说明从节点在短线前同步的redis节点并不是当前的主节点，只能进行全量复制。

过程原理：

![QQ截图20210518202717](分布式微服务.assets/QQ截图20210518202717.png)

##### 分布式缓存寻址算法

* hash算法：根据key进行hash运算、结果对分片取模，确定分片

  适合固定分片数的场景

  扩展分片或者减少分片时，所有的数据需要重新计算分片、存储

* 一致性哈希：将整个Hash值的区间组成一个闭合圆环，计算每台服务器的Hash值、映射到圆环中。使用相同的hash算法计算数据的Hash值，映射到圆环，顺时针寻找，找到的第一个服务器就是数据存储的服务器。

  新增及减少节点只会影响节点到他逆时针最近的一个服务器之间的值

* Hash slot（哈希桶）：将数据与服务器隔离开，数据与slot映射，slot与服务器映射，数据进行hash决定存放的slot新增及删除节点时，将slot进行迁移即可

##### Redis高可用方案

主从

哨兵模式：

sentinel，烧饼是redis集群中非常重要的一个组件，主要有以下功能：

* 集群监控：负责监控redis master和slave进程是否正常工作
* 消息通知：如果某个redis实例有故障，那么烧饼负责发送消息作为报警给管理员。
* 故障转移：如果master node挂掉了，会自动转移到slave node上
* 配置中心：如果故障转移发生了，通知client客户端新的master地址

哨兵用于实现redis集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，相互协同工作。

* 故障转移时，判断一个master node是否宕机了，需要大部分的哨兵都同一才行，涉及到了分布式选举
* 即使部分哨兵节点挂掉了，烧饼集群还是能正常工作的
* 哨兵通常需要三个实例，来保证自己的健壮性
* 哨兵+redis主从的部署结构，是不保证数据丢失的，只能保证redis集群的高可用性
* 对于哨兵+redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练



Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。采用slot（槽）的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询发送到正确的节点上执行。

方案说明：

* 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽（哈希值）区间的数据，默认分配16384个槽位
* 每份数据分片会存储在多个互为主从的多节点上
* 数据写入先写主节点，再同步到从节点上
* 数据写入主节点，再同步到从节点（支持配置为阻塞同步）
* 同一分片多个节点间的数据不保持强一致性
* 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点
* 扩容需要把旧的节点的数据的一部分迁移到新的节点

在Redis cluster架构下，每个redis要开放两个端口，比如一个是6379，另一个是加1w的端口16379

16379端口号是用来进行节点间通信的，也就是cluster bus的通信，用来进行故障检测、配置更新、故障转移授权。Cluster bus采用另一种二进制的协议，gossip协议，用于节点间进行高效的数据转换，占用更少的网络带宽和处理时间。

优点：

* 无中心架构，支持动态扩容，对业务透明
* 具备Sentinel的监控和自动Failover（故障转移）能力
* 客户端不需要链接集群所有的节点，链接集群中的任何一个节点即可
* 高性能，客户端直连redis服务，免去了proxy代理的损耗

缺点：

* 运维也很复杂，数据迁移需要人工干预
* 只能使用0号数据库
* 不支持批量操作（pipeline管道操作）
* 分布式逻辑和存储模块耦合等



Redis Sharding是在Redis Cluster出来之前，业界普遍使用的多redis实例集群方法。其主要思想是采用哈希算法将redis数据的key进行散列，通过hash函数，特定的key会映射到特定的redis节点上。Java redis客户端驱动jedis，支持redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool

优点：

优势在于非常的简单，服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强。

缺点：

由于sharding处理放到客户端，规模进一步扩大时给运维带来挑战。

客户端Sharding不支持动态增删节点，服务端Redis实例集群拓扑结构有变化的时候，每个客户端都需要更新调整。链接不能共享，当规模增大时，资源浪费制约优化。



##### Redis事物实现

1.事物开始

MULTI命令的执行，标识一个事物的开始。MULTI命令会将客户端状态的flags属性中打开REDIS_MULTI标识来完成。

2.命令入队

当一个客户端切换到事物状态之后，服务器会根据这个客户端发送来的命令来执行不同的操作。如果客户端发送的命令为MULTI、EXEC、WATCH、DISCARD中的一个，立即执行这个命令，否则将命令放入一个事物队列里面，然后向客户端返回QUEUED回复

* 如果客户端发送的命令为EXEC、DISCARD、WATCH、MULTI命令中的一个，那么服务器立即执行这个命令

* 如果客户端发送的是四个命令以外的其他命令，那么服务器并不执行这个命令。

  首先检查此命令的格式是否正确，如果不正确，服务器会在客户端状态（redisClient）的flags属性关闭REDIS_MULTI标识，并且返回错误信息给客户端。

  如果正确，将这个命令放入一个事物队列里面，然后向客户端返回QUEUED回复

事物队列是按照FIFO的方式保存入队的命令

3.事物执行

客户端发送EXEC命令，服务器执行EXEC命令逻辑。

* 如果客户端状态的flag属性不包含REDIS_MULTI表示，或者包含REDIS_CAS或者REDIS_DIRTY_EXEC表示，那么就直接取消事物的执行。
* 否则客户端处于事物状态（flags有REDIS_MULTI标识），服务器会遍历客户端的事物队列，然后执行事物队列中的所有命令，最后将返回结果全部返回给客户端；

Redis不支持事物回滚机制，但是它会检查每一个事物中的命令知否错误。

Redis事物不支持检查那些程序员自己逻辑错误，例如对String类型的数据库键执行HashMap类型的操作！



* WATCH命令是一个乐观锁，可以为redis事物提供check-and-set（CAS）行为。可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事物就不会执行，监控一致持续到EXEC命令
* MULTI命令用于开启一个事物，它总是返回OK。MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。
* EXCE：执行所有事物快内的命令，返回事物块内所有命令的返回值，按命令执行的先后循序排列。当操作被打断时，返回nil
* 通过调用DISCARD，客户端可以清空事物队列，并放弃执行事物，并且客户端会从事物状态中退出。
* UNWATCH命令可以取消watch对所有key的监控

##### Redis数据结构

String：字符串

List：列表

Hash：哈希表

Set：无序集合

Sorted Set：有序集合

Bitmap：布隆过滤器

GeoHash：坐标，借助Sorted Set实现，通过zset的score进行排序就可以得到坐标附近的其他元素，通过将score还原成坐标值就可以得到元素的原始坐标

HyperLogLog：统计不重复数据，用于大数据基数统计

Streams：内存版kafka

##### 数据库实现分布式锁的问题以及解决方案

利用唯一约束键存储key，insert成功则代表获取锁成功，失败则获取锁失败，操作完成需要删除锁

问题：

* 非阻塞，获取锁之后没有排队机制，需要自己编码实现阻塞，可以使用自旋锁，直到获取锁
* 不可重入，如果加锁方法需要递归，则第二次插入会失败，可以使用记录线程标记（如uuid）解决重入问题
* 死锁，删除锁失败、则其他线程没办法获取锁，可以设置超时时间、使用定时任务检查
* 数据库单点故障，数据库高可用

##### Redis分布式锁实现

setnx+setex：存在设置超时时间失败的情况，导致死锁（非原子操作）

set(key,value,nx,px)：将setnx+setex变成原子操作

问题：

* 任务超时，锁自动释放，导致并发问题。使用redisson解决（看门狗监听，自动续期）
* 以及加锁和释放锁不是同一个线程的问题。在value中存入uuid（线程唯一标识）、删除锁时判断该标识（使用lua保证原子操作）
* 不可重入性，使用resission解决（实现机制类似AQS，计数）
* 异步复制可能造成锁丢失，使用redLock解决
  1. 顺序向5个节点请求加锁
  2. 根据一定的超时时间来判断是不是跳过该节点
  3. 3个节点加锁成功并且花费时间小于锁的有效时间
  4. 认定加锁成功

### MQ

##### 消息队列的优缺点，使用场景

优点：

1. 解耦，降低系统之间的依赖
2. 异步处理，不需要同步等待
3. 削峰填谷，将流量从高峰期引到低谷进行处理

缺点：

1. 增加了系统的复杂度、幂等、重复消费、消息丢失等问题的带入。
2. 系统可用性降低、mq的故障会影响系统可用
3. 一致性，消费端可能失败

场景：日志收集、发布订阅（分布式下的观察者）

##### 保证消息不被重复消费、幂等性

幂等：一个数据或者一个请求，重复来多次，确保对应的数据是不会改变的，不能出错。

思路：

* 如果是写redis，就没问题，反正每次都是set，天然幂等性
* 生产者发送消息的时候带上一个全局唯一的id，消费者拿到消息后，先根据这个id去redis里查一下，之前有没有消费过，没有消费过就处理，并且写入这个id到redis，如果消费过了，则不处理。
* 基于数据库的唯一主键

##### Kafka、ActiveMQ、RabbitMQ、RocketMQ对比

ActiveMQ：JMS规范，支持事物、支持XA协议，没有生产大规模支撑场景、官方维护越来越少

RabbitMQ：erlang语言开发、性能好、并发高、支持多种语言，社区、文档方面有优势，erlang语言不利于Java程序员二次开发，以来开源社区的维护和升级，需要学习AMQP协议、学习成本相对较高

以上吞吐量单位都在万级

kafka：高性能、高可用，生产环境有大规模使用场景，单机容量有限（超过64个分区相应明显边长）、社区更新慢

吞吐量单机百万

rocketmq：Java实现，方便二次开发、设计参考kafka，高可用、高可靠，社区活跃度一般、支持语言较少吞吐量单机十万

##### RabbitMQ架构设计

producer：生产者

Connection：链接、channel：信道

broker：节点、exchange：交换机、queue：队列、binding key（route key）：路由key

consumer：消费者

##### RabbitMQ的交换机类型

交换机分发会先找到绑定的队列，然后再判断rotekey，来决定是否将消息分发到某一个队列中：

* fanout：扇形交换机，不再判断routekey、直接将消息分发到所有绑定的队列
* direct（默认）：判断routekey的规则是完全匹配模式，即发送消息时指定routekey要等于绑定的routekey
* topic：判断routekey的规则是模糊匹配模式
* header：绑定队列与交换机的时候指定一个键值对，当交换机在分发消息的时候会先解开消息体里的headers数据，然后判断里面是否有所设置的键值对，如果发现匹配成功，才将消息分发到队列中；这种交换器类型在性能上相对来说较差，在实际工作中很少用到。

##### rabbitmq可以直连队列吗？

其实从客户端的角度来看是可以的，你可以在发送消息的时候只指定队列而不指定交换机。但是mq服务器接收的过程中是会走一个默认的交换机的。就是一个交换机来做投递这个事情。

##### rabbitMQ的几种通信模型

1、简单模式 HelloWorld

一个生产者、一个消费者，不需要设置交换机（使用默认的交换机）

2、工作队列模式 Work Queue

一个生产者、多个消费者（竞争关系），不需要设置交换机（使用默认的交换机）

3、发布订阅模式 Publish/subscribe

需要设置类型为fanout的交换机，并且交换机和队列进行绑定，当发送消息到交换机后，交换机会将消息发送到绑定的队列

4、路由模式 Routing

需要设置类型为direct的交换机，交换机和队列进行绑定，并且指定routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列

5、通配符模式 Topic

需要设置类型为topic的交换机，交换机和队列进行绑定，并且指定通配符方式的routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列

##### rabbitMQ的持久化机制

1. 交换机持久化：exchange_declare创建交换机时通过参数指定
2. 队列持久化：queue_declare创建队列时通过参数指定
3. 消息持久化：new AMQPMessage创建消息时通过参数指定

append的方式写文件，会根据大小自动生成新的文件，rabbitMQ启动时会创建两个进程，一个负责持久化消息的存储，另一个负责非持久化消息的存储（内存不够时）

雄安锡存储时会在ets表中记录消息在文件中的映射以及相关信息（包括id、偏移量、有效数据、左边文件、右边文件），消息读取时根据该信息到文件中读取、同时更新信息。

消息删除时只从est删除，变为垃圾数据，当垃圾数据超过比例（默认50%），并且文件数达到3个，触发垃圾回收，锁定左右两个文件，整理左边文件有效数据、将右边文件有效数据写入左边，更新文件信息，删除右边，完成合并。当一个文件的有效数据等于0时，删除该文件



写入文件前写入buffer缓冲区，如果buffer已满，则写入文件（此时只是操作系统的页存  page cache）

每隔25ms刷一次磁盘，不管buffer满没满，都将buffer和页存中的数据落盘

每次消息写入后，如果没有后续写入请求，则直接刷盘



##### RabbitMQ事物消息

通过对信道设置实现

1. channel.txSelect（）；通知服务器开启事物模式；服务端会返回Tx.Select-OK
2. channel.basicPublish；发送消息，可以是多条，可以是消费消息提交 ack
3. channel.txCommit（）提交事物
4. channel.txRollback（）回滚事务

MQ上存在一个事物队列，当提交事物的时候才会把事物中的消息转到目标队列中，等待消费者消费。

消费者使用事物：

1. autoACK=false，手动提交ack，以事物提交或回滚为准；
2. autoACK=ture，不支持事物的，也就是说你即使在收到消息之后再回滚也是无济于事的，队列已经把消息移除了

如果其中任意一个环节出现问题，就会抛出IoException异常，用户可以拦截异常进行事物回滚，或决定要不要重复消息。事物消息会降低rabbitmq的性能。事物是同步的，ack是异步的。

##### RabbitMQ如何保证消息的可靠性传输

1. 使用事物消息
2. 使用消息确认机制

MQ的可靠性=传输可靠性+持久化

* channel设置成confirm模式，则每条消息会被分配一个唯一id
* 消息投递成功，信道会发送ack给生产者，包含了id，回调ConfirmCallback接口
* 如果发生错误导致消息丢失，且只有一次，异步触发。可以继续发送消息。

接收方确认：

* 声明队列时，指定noack=false，broker会等待消费者手动返回ack，才会删除消息，否则立刻删除
* broker的ack没有超时机制，只会判断链接是否断开，如果断开、消息会被重新发送

##### rabbitMQ的死信队列、延迟队列原理

死信消息：

1. 消息被消费方否定确认，使用channel.basicNack或channel.basicReject，并且此时requeue属性被设置成false
2. 消息在队列的存活时间超过设置的TTL时间
3. 消息队列的消息数量已经超过了最大的队列长度

那么该消息将成为死信消息。如果配置了死信队列信息，那么该消息将会被丢尽死信队列中，如果没有配置，则该消息会被丢弃

为每个需要使用死信队列的业务队列配置一个死信交换机，同一个项目的死信交换机可以共享一个，然后为每个业务队列分配一个单独的routekey，死信队列只不过是绑定在死信交换机上的队列，私信交换机也不是什么特殊的交换机，只不过是用来接收死信的交换机，所以可以为任何类型【direct、fanout、topic】



TTL：一条消息或者该队列中的所有消息的最大存活时间

如果一条消息设置了TTL属性或者进入了设置TTL属性的队列，那么这条消息如果在TTL设置的时间内没被消费，则会成为“死信”。如果同时配置了队列的TTL和消息的TTL，那么较小的那个值将会被使用。

##### rabbitMQ的普通集群模式

![QQ截图20210523160439](分布式微服务.assets/QQ截图20210523160439.png)

保证不了高可用，真实的消息信息没有备份。

作用因为一个服务器的性能只有一万，这样横向扩展之后可以处理更多的请求。每个节点都支持读写操作。

元数据：

* 队列元数据：队列名称和它的属性
* 交换机元数据：交换机名称、类型、属性
* 绑定元数据：一个简单的表格展示了如何将消息路由到队列
* vhost元数据：为vhost内的队列、交换机和绑定提供命名空间和安全属性

为什么只同步言数据：

* 存储空间，每个节点都保存全量数据，影响消息堆积能力
* 性能，消息的发布者需要将消息复制到每一个集群节点

客户端链接的是非队列数据所在的节点：则该节点会进行路由转发，包括发送和消费

集群节点类型：

* 磁盘系欸但：将配置信息和元数据信息存储在磁盘上
* 内存节点：将配置信息和元数据存储在内存中。性能优于磁盘节点。依赖磁盘节点进行持久化。

RabbitMQ要求集群中至少有一个磁盘节点，当节点加入和离开集群时，必须通知磁盘节点（如果集群中唯一的磁盘节点崩溃了，则不能进行创建队列、创建交换器、创建绑定、添加用户、更改权限、添加和删除集群节点）。如果唯一磁盘的磁盘节点崩溃，集群时可以保持运行的，但是不能更改任何东西。因此建议在集群中设置两个磁盘节点，只要一个可以，就能正常操作。



高可用可以自己做架构

##### Kafka的架构设计

![QQ截图20210523181445](分布式微服务.assets/QQ截图20210523181445.png)

Consumer group：消费者组，消费者组内每个消费者负责消费不同分区的数据，提高消费能力。逻辑上的一个订阅者。

Topic：可以理解为一个队列，Topic将消息分类，生产者和消费者面向的是同一个Topic。

Partition：为了实现扩展性，提高并发能力，一个Topic以多个Partition的形式分布在多个Broker上，每个Partition是一个有序的队列。一个Topic的每个Partition都有若干个副本（Replica），一个Leader和若干个Follower。生产者发送数据的对象，以及消费者消费数据对象，都是Leader。Follwer负责实时从Leader中同步数据，保持和Leader数据的同步。Leader发生故障时，某个Folleower还会成为新的Leader。

##### Kafka高性能高吞吐的原因

1、磁盘顺序读写：保证了消息的堆积

* 顺序读写，磁盘会预读，预读即再读取的起始地址连续读取多个页面，主要时间花费在传输时间，而这个时间两种读写可以认为是一样的。
* 随机读写，因为数据没有在一起，将预读浪费掉了。需要多次寻道和旋转延迟。而这个时间可能是传输时间的许多倍。

2、零拷贝：避免CPU将数据从一块存储拷贝到另一块存储的技术

* 传统的数据复制：

  1. 读取磁盘文件到内核缓冲区
  2. 将内核缓冲区的数据copy到用户缓冲区
  3. 将用户缓冲区的数据处理完毕copy到socket的发送缓冲区
  4. 将socket发送缓冲区的数据发送到网卡、进行传输

* 零拷贝

  磁盘文件——>内核空间读取缓冲区——>网卡接口——>消费者进程

零拷贝：因为kafka作为一个消息中间件，其作用并没有对消息的进一步加工所以可以使用。而我们写的应用程序则不可以。操作系统分为内核态和用户态，用户态无法操作硬件资源，比如磁盘、网卡，并且各自有自己的线程。消息的读取和转发过程并不需要加工。所以kafka直接将磁盘中的数据读到网卡发送，避免了内存态的拷贝。

3、分区分段+索引

​	kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，kafka又默认为分段后的数据建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。

4、批量压缩：多条消息一起压缩，降低带宽

5、批量读写、批量压缩、批量传输，客户端得到后再解压缩。

6、直接操作page cache，而不是JVM，避免GC耗时及对象创建耗时，且写速度更高，进程重启、缓存也不会丢失

##### Kafka的副本同步机制

![QQ截图20210523184627](分布式微服务.assets/QQ截图20210523184627.png)



LFO：下一条待写入的位置

HW：同步标识（存在短板效应）ISR表中最小的代表当前分区的最大已同步

firstUnstableOffset：第一条未提交的数据

LastStableOffset：最后一条已提交数据

LogStartOffset：起始位置

isolation.level=read_committed：只能消费到LastStableOffset，read_committed可以消费到HW的上一条

一个partition对应的ISR中最小的LEO作为分区的HW，consumer最多只能消费到HW所在的位置

Leader收消息后会更新本地LEO，leader还会维护Follower的LFO即remote LEO，follower发出fetch同步数据请求时（携带自身LEO）、Leader会更新remote LEO，更新分区的HW，然后将数据相应给follower、follower更新自身HW（取响应中的HW和自身的LEO中的较小值），LEO+1

ISR：如果一个follwer落后leader不超过某个时间阈值，那么则在ISR中，否则将放在OSR中。



同步副本时，follower获取leader的LEO和LogStartOffset，与本地对比、如果本地的LogStartOffset超出了leader的值，则超过这个值的数据删除，再同步，如果本地的小于leader的则直接同步

##### Kafka消息可靠性的解决方案

出现原因：主从同步的过程中主节点宕机导致数据丢失。

消息发送：

* 设置ack：0：不重试、1：leader写入成功就返回、all/-1：等待ISR同步完再返回
* 禁止选举ISR之外的follower为leader，unclean.leader.election.enable：false
* tries>1，重试次数
* min.insync.replicas>1：同步副本数，没满足该值之前、不提供读写服务、读写操作会异常

消费：

* 手动提交offset
* broker：减少刷盘间隔
* 事物消息

### end





高可用、分区容错性、数据一致性——>分布式提出的要求

高可靠——>第三方插件保护数据不丢失



